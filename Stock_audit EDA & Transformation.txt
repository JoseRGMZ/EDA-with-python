## Archivo Auditoria
import pandas as pd
import os
import numpy as np
import re
from collections import Counter
from datetime import datetime
import locale
import random
import string

# Configurar el locale a español (esto varía según el sistema operativo)
try:
    locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')  # Para sistemas Unix/Linux
except locale.Error:
    try:
        locale.setlocale(locale.LC_TIME, 'Spanish_Spain.1252')  # Para Windows
    except locale.Error:
        print("No se pudo establecer la configuración regional a español. Verifica tu sistema.")

# Paso 1: Cargar datos desde un archivo CSV
def load_data(file_path):
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path, encoding='latin1')
            print("Archivo auditoria cargado correctamente.")
            return df
        except UnicodeDecodeError as e:
            print(f"Error al leer el archivo: {e}")
            return None
    else:
        print(f"El archivo no se encuentra en la ruta: {file_path}")
        return None

# Función para generar un prefijo aleatorio del abecedario
def generate_random_prefix(length=4):
    return ''.join(random.choices(string.ascii_uppercase, k=length))

def generate_prefix_with_frequency(length=4, target_frequency=0.33):
    # Generar prefijos con una probabilidad controlada
    return generate_random_prefix() if random.random() >= target_frequency else generate_random_prefix()

# Paso 3: Añadir columna de AUDIT_ID única basada en la fecha actual y prefijos aleatorios
def add_audit_id(df):
    fecha_actual = datetime.now().strftime('%Y%m%d')
    df['AUDIT_ID'] = [
        generate_prefix_with_frequency(length=4, target_frequency=0.23) + fecha_actual + str(i + 1)
        for i in range(len(df))
    ]
    print("\nColumna 'AUDIT_ID' añadida con éxito.")
    return df

# Paso 4: Realizar el análisis exploratorio de los datos (EDA)
def EDA(df):
    # Reemplazar todas las instancias de 'No hay datos' en el DataFrame por NaN
    df.replace('No hay datos', np.nan, inplace=True)
    print("\nTodas las instancias de 'No hay datos' en el DataFrame se han reemplazado por NaN.")

    # Convertir la columna 'DAY EVENT' al formato DD/MM/YYYY
    df = reemplazar_abreviaturas_mes(df, 'DAY EVENT')

    # Verificar la columna 'PROCESO' y reemplazar 'No hay datos' por NaN
    if 'PROCESO' in df.columns:
        df['PROCESO'] = df['PROCESO'].replace('No hay datos', np.nan)
        print("\nValores 'No hay datos' en la columna 'PROCESO' convertidos a NaN.")
        
        # Validar si hay valores fuera del conjunto esperado
        valores_validos = ['disposal', 'transfer', 'order', 'withdrawal']
        valores_invalidos = df[~df['PROCESO'].isin(valores_validos) & df['PROCESO'].notna()]
        if not valores_invalidos.empty:
            print("\nValores inválidos encontrados en la columna 'PROCESO':")
            print(valores_invalidos['PROCESO'].drop_duplicates())
        else:
            print("\nTodos los valores en la columna 'PROCESO' son válidos o NaN.")

    escanear_estructuras(df)

    print("\nPrimeras 5 filas del dataset:")
    print(df.head())

    print("\nEstadísticas descriptivas:")
    print(df.describe(include='all'))

    print("\nInformación del DataFrame:")
    print(df.info())

    print("\nValores nulos en cada columna:")
    print(df.isnull().sum())

    print("\nNúmero de filas duplicadas antes de eliminar:")
    print(df.duplicated().sum())
    df.drop_duplicates(inplace=True)
    print("\nNúmero de filas duplicadas después de eliminar:")
    print(df.duplicated().sum())

    return df

# Paso 5: Escanear y valorar las estructuras de cada columna
def escanear_estructuras(df):
    columnas_excepcion = ['LDAP/REP', 'TYPE', 'TIPO', 'OFFENDER PROCESS']

    def identificar_estructura(valor):
        if pd.isna(valor):
            return 'NaN'
        if re.match(r'^[a-záéíóúñ]+(\s[a-záéíóúñ]+)*$', str(valor).lower()):
            return 'Nombre completo'
        if re.match(r'^\d+$', str(valor)):
            return 'Número'
        if re.match(r'^MZ-\d{1,2}-\d{3}-\d{2}-\d{2}$', str(valor)):
            return 'Formato ORIGEN'
        return 'Otro'

    resultados_estructuras = {}

    for columna in df.columns:
        if columna in columnas_excepcion:
            print(f"\nLa columna '{columna}' se ha excluido de la revisión.")
            continue

        estructuras = df[columna].apply(identificar_estructura)
        conteo_estructuras = Counter(estructuras)
        
        if 'Nombre completo' in estructuras.values:
            df.loc[estructuras == 'Nombre completo', columna] = df.loc[estructuras == 'Nombre completo', columna].apply(
                lambda x: ' '.join([x.split()[0].title()] + [part.title() for part in x.split()[1:]]) if pd.notna(x) else x
            )

        resultados_estructuras[columna] = conteo_estructuras

        print(f"\nEstructuras encontradas en la columna '{columna}':")
        for estructura, conteo in conteo_estructuras.items():
            porcentaje = (conteo / len(df)) * 100
            print(f" - {estructura}: {conteo} registros ({porcentaje:.2f}%)")
        
        estructura_comun = conteo_estructuras.most_common(1)[0][0]
        print(f"Estructura predominante en '{columna}': {estructura_comun}")

        if estructura_comun == 'Otro':
            ejemplo_otro = df[df[columna].apply(identificar_estructura) == 'Otro'][columna].sample(1).values[0]
            print(f"Ejemplo de un valor con la estructura 'Otro' en '{columna}': {ejemplo_otro}")
        
    return resultados_estructuras

# Paso 6: Guardar el DataFrame actualizado
def save_updated_data(df, file_path):
    new_file_path = file_path.replace('.csv', '_actualizado.csv')
    try:
        df.to_csv(new_file_path, index=False, encoding='latin1')
        print(f"\nEl DataFrame actualizado se ha guardado correctamente en: {new_file_path}")
    except Exception as e:
        print(f"\nError al guardar el archivo: {e}")

# Main para ejecutar el análisis
def main():
    file_path = r'C:\Users\463847\Downloads\2.csv'  ## agregar path en formato crudo dependiendo directorio local del input

    df = load_data(file_path)
    
    if df is not None:
        df = add_audit_id(df)
        df = EDA(df)
        save_updated_data(df, file_path)

if __name__ == "__main__":
    main()